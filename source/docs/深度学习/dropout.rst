Dropout
=======

2012年Hinton在论文《Improving neural networks by preventing
co-adaptation of feature detectors》中提出Dropout，用于解决过拟合问题。

对Dropout算法行之有效的解释较为模糊，原论文中Hinton认为Dropout向神经网络中加入了随机性，降低了节点之间的强相关，也有人将Dropout理解为某种模型平均。我们不得不承认深度学习可解释性问题依然是阻碍其发展的最大问题，当前的深度学习依然是实验科学，缺乏严谨的理论模型的支撑。

算法描述
--------

Dropout作用于某个计算层，让其中每个“神经元”具有一定概率p不激活，即不参与计算，这就是Dropout的全部内容

实现
----

深度神经网络每一层的计算都可以视为多个线性函数的复合，现假设有节点D，其下一层某节点的计算值可以表示为

.. math::


   Z_{i}^{l+1}=w_{i}^{l+1}y_{d}^{l}+...+b_{i}^{l+1} \\
   y_{d}^{l}为节点D的值

若节点D被判定为不激活，即D节点不参与计算，我们可以将D节点的值设置为0，由于Dropout只作用于训练过程，且对节点的判定是随机的，所以0值的设定是暂时的，只作用于当前轮次

好像Dropout的实现说完了，但我们忽略了一个重要的细节，当我们引入概率p使得部分节点不激活后，导致了计算结果数据的分布发生变化。现假设某层L计算结果的期望是a，当引入概率p后

.. math::


   E(L)=a \\
   E(L_{p})=(1-p)*a

我们应该很清楚数据分布的变化对深度模型的训练会带来什么，多种正则化方法都为了避免训练过程导致分布变化，而Dropout却引入了这样的问题，我们当然需要解决

既然期望变为了(1-p)*a那么我们人为对其进行缩放

.. math::


   scale=\frac{1}{1-p} \\

对每一个节点的计算结果都乘一个scale进行缩放，那么这一层的期望又变回了a

.. math::


   E(L_p)=(1-p)a * \frac{1}{1-p}=a

梯度
----

当节点判定为不激活时，梯度为0，当判定为激活时，梯度为scale
